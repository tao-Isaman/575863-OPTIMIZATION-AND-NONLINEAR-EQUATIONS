# -*- coding: utf-8 -*-
"""Optimizer (Gradient Desent)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orIQkUXrqzGBFBt6zvGBNUsksabU78Zp
"""

from numpy import cos

def derivative(input_fuction, x):
  '''
  return derivative of function of x
  '''
  h = 0.0001
  return (input_fuction(x + h) - input_fuction(x)) / h

def tao_optimizer(input_function, next_x, optimizer, firstDistance):
  next_distance = derivative(input_function,next_x)
  print(firstDistance , next_distance)
  return optimizer * (1 + (next_distance/firstDistance))

def find_minima(input_function,x0,round,optimizer) :
  '''
  find x where x is give a minimum value of function
  '''
  loss = []
  current_x = x0
  learnign_rate = optimizer
  firstDistance = derivative(input_function,current_x)
  for i in range(round):
    next_x = current_x - learnign_rate * derivative(input_function , current_x)
    loss.append(next_x)
    if (next_x == current_x):
      print("x is not change")
      return loss
    
    # change optimizer
    learnign_rate = tao_optimizer(input_function,next_x, learnign_rate , firstDistance)

    # change x
    current_x = next_x

    print("learnign_rate = " ,learnign_rate)
    print("evaluate = ", derivative(input_function,current_x))
    print(i, current_x)

  return loss

# objective_function = lambda x: 5*x**2 - 2*x - 1
objective_function = lambda x: (5*x**2) - (5*x) + 1
x = find_minima(objective_function, 5, 1000, 0.01)
print(x[-1])
derivative(objective_function, x[-1])

import matplotlib.pyplot as plt

plt.plot(x)